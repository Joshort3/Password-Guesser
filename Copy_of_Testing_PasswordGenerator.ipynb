{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Testing PasswordGenerator.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnVYpujPRW3i",
        "outputId": "97ade76f-4ba7-4699-fe0c-a5cf13263d27"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls \"/content/drive/Shareddrives/Data Analytics in Cybersecurity Team\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            " 10k-most-common.txt\n",
            " 10-million-password-list-top-10000.txt\n",
            "'An Online Password Guessing Method Based on Big Data.pdf'\n",
            "'A password generator tool to increase users awareness on bad password construction strategies.pdf'\n",
            " CSCE489_704_Final_Project_Spring_2021.docx\n",
            " data.csv\n",
            " output_file.txt\n",
            " passlist.csv\n",
            " PasswordClassifier.ipynb\n",
            "'Password Generation and Strength Prediction Presentation.gslides'\n",
            " PasswordGenerator.ipynb\n",
            "'Password Strength Prediction Using Supervised Machine Learning Techniques.pdf'\n",
            "'Project Checkpoint 1.gdoc'\n",
            "'Project Checkpoint 2.gdoc'\n",
            "'Project Proposal.gdoc'\n",
            "'Research Paper.gdoc'\n",
            " top204000passwords.txt\n",
            " top4800passwords.txt\n",
            " top64passwords.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJEqxnNORHoR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c22a9088-32a7-4435-db72-b9ba872397c0"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRvvifH4R-10",
        "outputId": "97a3c15a-b610-420e-d531-28f9d09eb73f"
      },
      "source": [
        "path = '/content/drive/Shareddrives/Data Analytics in Cybersecurity Team/top4800passwords.txt'\n",
        "text = open(path, 'rb').read().decode(encoding='utf-8')\n",
        "allchars = \"qwertyuiopasdfghjklzxcvbnm1234567890-=!@#$%^&*()_+[];',./{}:\\\"<>?|\\\\~`\"\n",
        "print(f'Length of text: {len(text)} characters')\n",
        "allchars = sorted(set(text))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 50074 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsEekpWETBaL"
      },
      "source": [
        "# Convert characters to number IDs\n",
        "ids_from_chars = preprocessing.StringLookup(\n",
        "    vocabulary=list(allchars))\n",
        "# convert number IDs to characters\n",
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True)\n",
        "# Function to convert number IDS into string\n",
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xaZqT61UCh2",
        "outputId": "515f12e8-8f66-4425-83f7-ed7507482060"
      },
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "for ids in ids_dataset.take(13):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "p\n",
            "a\n",
            "s\n",
            "s\n",
            "w\n",
            "o\n",
            "r\n",
            "d\n",
            "\r\n",
            "\n",
            "\n",
            "1\n",
            "2\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IW6BtxDcWkVJ",
        "outputId": "4f16c159-9a1a-425e-e72a-cdc468c66c2d"
      },
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "print(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<MapDataset shapes: ((100,), (100,)), types: (tf.int64, tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDQEeAntbYSB",
        "outputId": "ea07d3e0-cb8d-4302-a9b0-38c0ed84e47e"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((32, 100), (32, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7UGrSbRbqgf"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(allchars)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024\n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.gru2 = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LrO_3tjcKjx"
      },
      "source": [
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWGdP3uNctmY",
        "outputId": "bfff6bd9-a875-42ac-f4ab-7919f865ecfe"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 100, 60) # (batch_size, sequence_length, vocab_size)\n",
            "Input:\n",
            " b'era\\r\\nabundance\\r\\nNicholas\\r\\nMichael1\\r\\n9999999999\\r\\n1qazzaq1\\r\\n13243546\\r\\n12qw34er\\r\\n123456ab\\r\\nzaragoza\\r\\nwo'\n",
            "\n",
            "Next Char Predictions:\n",
            " b'noRjPDfy8hNiRdW2F@lk@c1wgvphSv4kM0ABfvxB5h0r@n\\rARVWpFjOj9zd52lIoIkS@IAlpdGqxVDLoiw7wO[UNK]7m;3Ifzgs3aO2'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmQ0nhLidFao",
        "outputId": "d25add24-e6e1-4554-e33c-56443c949b05"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "EPOCHS = 30\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "15/15 [==============================] - 3s 99ms/step - loss: 3.8979\n",
            "Epoch 2/30\n",
            "15/15 [==============================] - 1s 91ms/step - loss: 3.0523\n",
            "Epoch 3/30\n",
            "15/15 [==============================] - 1s 89ms/step - loss: 2.6463\n",
            "Epoch 4/30\n",
            "15/15 [==============================] - 1s 89ms/step - loss: 2.4634\n",
            "Epoch 5/30\n",
            "15/15 [==============================] - 1s 89ms/step - loss: 2.3212\n",
            "Epoch 6/30\n",
            "15/15 [==============================] - 1s 90ms/step - loss: 2.2425\n",
            "Epoch 7/30\n",
            "15/15 [==============================] - 1s 89ms/step - loss: 2.1930\n",
            "Epoch 8/30\n",
            "15/15 [==============================] - 1s 90ms/step - loss: 2.1576\n",
            "Epoch 9/30\n",
            "15/15 [==============================] - 1s 89ms/step - loss: 2.1362\n",
            "Epoch 10/30\n",
            "15/15 [==============================] - 1s 90ms/step - loss: 2.1145\n",
            "Epoch 11/30\n",
            "15/15 [==============================] - 1s 91ms/step - loss: 2.0995\n",
            "Epoch 12/30\n",
            "15/15 [==============================] - 1s 90ms/step - loss: 2.0725\n",
            "Epoch 13/30\n",
            "15/15 [==============================] - 1s 90ms/step - loss: 2.0665\n",
            "Epoch 14/30\n",
            "15/15 [==============================] - 1s 90ms/step - loss: 2.0438\n",
            "Epoch 15/30\n",
            "15/15 [==============================] - 1s 91ms/step - loss: 2.0196\n",
            "Epoch 16/30\n",
            "15/15 [==============================] - 1s 91ms/step - loss: 1.9983\n",
            "Epoch 17/30\n",
            "15/15 [==============================] - 1s 91ms/step - loss: 1.9877\n",
            "Epoch 18/30\n",
            "15/15 [==============================] - 1s 90ms/step - loss: 1.9519\n",
            "Epoch 19/30\n",
            "15/15 [==============================] - 1s 90ms/step - loss: 1.9376\n",
            "Epoch 20/30\n",
            "15/15 [==============================] - 1s 91ms/step - loss: 1.9003\n",
            "Epoch 21/30\n",
            "15/15 [==============================] - 1s 90ms/step - loss: 1.8650\n",
            "Epoch 22/30\n",
            "15/15 [==============================] - 1s 90ms/step - loss: 1.8306\n",
            "Epoch 23/30\n",
            "15/15 [==============================] - 1s 91ms/step - loss: 1.7939\n",
            "Epoch 24/30\n",
            "15/15 [==============================] - 1s 91ms/step - loss: 1.7584\n",
            "Epoch 25/30\n",
            "15/15 [==============================] - 1s 91ms/step - loss: 1.7221\n",
            "Epoch 26/30\n",
            "15/15 [==============================] - 1s 90ms/step - loss: 1.6748\n",
            "Epoch 27/30\n",
            "15/15 [==============================] - 1s 91ms/step - loss: 1.6405\n",
            "Epoch 28/30\n",
            "15/15 [==============================] - 1s 90ms/step - loss: 1.5987\n",
            "Epoch 29/30\n",
            "15/15 [==============================] - 1s 90ms/step - loss: 1.5312\n",
            "Epoch 30/30\n",
            "15/15 [==============================] - 1s 90ms/step - loss: 1.4887\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFEWTgFDecNp"
      },
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['', '[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grhc4NQpeeSd"
      },
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaXwrcPdexa9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e4af1c1-d32c-4f8c-f68f-4636b3148046"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['pas'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pass\r\n",
            "davinaus\r\n",
            "crumborle\r\n",
            "chinitar\r\n",
            "caslinal\r\n",
            "cantroot\r\n",
            "barriabax\r\n",
            "appardeon\r\n",
            "athenlian\r\n",
            "a102a324\r\n",
            "1a1x2c123\r\n",
            "windechon\r\n",
            "versenes\r\n",
            "vingblose\r\n",
            "vematora\r\n",
            "opellome\r\n",
            "mariatova\r\n",
            "kanalova\r\n",
            "langagee\r\n",
            "levelduc\r\n",
            "raisher1\r\n",
            "redrelson\r\n",
            "ropedest\r\n",
            "refforder\r\n",
            "repordce\r\n",
            "rockert1\r\n",
            "milligal\r\n",
            "mandusin\r\n",
            "dleffosh\r\n",
            "clamball\r\n",
            "comanchtons\r\n",
            "cacrielda\r\n",
            "chappine\r\n",
            "calovial\r\n",
            "caurwbet\r\n",
            "cacamone\r\n",
            "enovakin\r\n",
            "christian\r\n",
            "changhas\r\n",
            "calorant\r\n",
            "conistra\r\n",
            "calefers\r\n",
            "concrunce\r\n",
            "clatemis\r\n",
            "chorling\r\n",
            "camestan\r\n",
            "caccesane\r\n",
            "cabriftro\r\n",
            "chomatox\r\n",
            "bordiald\r\n",
            "bisefann\r\n",
            "beliermon\r\n",
            "bablands\r\n",
            "babcoballa\r\n",
            "abracides\r\n",
            "abriness\r\n",
            "astardich\r\n",
            "ancharce\r\n",
            "asdffra1\r\n",
            "wercemen\r\n",
            "20020001\r\n",
            "windivin\r\n",
            "walmcisa\r\n",
            "wallo123\r\n",
            "vessions\r\n",
            "pendoure\r\n",
            "playquelen\r\n",
            "netperin\r\n",
            "muncuious\r\n",
            "mardigan\r\n",
            "lacklasd\r\n",
            "lif4fick\r\n",
            "loveline\r\n",
            "liferpiol\r\n",
            "lingslaph\r\n",
            "lairtome\r\n",
            "insedrit\r\n",
            "hobzybol\r\n",
            "halicala\r\n",
            "heariume\r\n",
            "gaterine\r\n",
            "adfidectir\r\n",
            "angolena\r\n",
            "alivence\r\n",
            "alchanger\r\n",
            "angelsen\r\n",
            "angyendel\r\n",
            "arabulla\r\n",
            "alucidet\r\n",
            "averter1\r\n",
            "asdfzosd\r\n",
            "wermering\r\n",
            "une4rmat\r\n",
            "antiancu\r\n",
            "Micrizan\r\n",
            "nathblah\r\n",
            "canauralo\r\n",
            "cas \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.334063291549683\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}